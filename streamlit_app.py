import streamlit as st
import yfinance as yf
import feedparser  # å°ˆæ¥­ RSS è§£æåº«ï¼Œé¿é–‹è¢«å°é–é¢¨éšª
import pandas as pd
from datetime import datetime

# --- 1. å®¢æˆ¶å€åŸŸï¼šåš´æ ¼ä¿ç•™å®Œç¾è¨­å®š (ä¸æ›´å‹•) ---
if 'clients' not in st.session_state:
    st.session_state.clients = {}

# (æ­¤è™•ä¿ç•™æ‚¨åŸæœ‰çš„ get_portfolio_report èˆ‡äº¤æ˜“ç´€éŒ„ UI ä»£ç¢¼)
# ... [ä¿ç•™åŸæœ‰çš„å®¢æˆ¶è³‡ç”¢è¨ˆç®—èˆ‡å´é‚Šæ¬„é‚è¼¯] ...

# --- 2. æ–°èå€åŸŸï¼šå…¨æ–° Feedparser å¼•æ“ (è§£æ±ºæŠ“ä¸åˆ°æ–°èçš„å•é¡Œ) ---
st.divider()
st.subheader("ğŸŒ å…¨çƒåœ°ç·£æ”¿æ²» & è²¡ç¶“ç›£æ§ (æ¬Šå¨åª’é«”å³æ™‚å°æ¥)")

def fetch_google_news_rss(keyword):
    """
    ä½¿ç”¨ feedparser ç›´æ¥æŠ“å– Google News RSSï¼Œç©©å®šæ€§æœ€é«˜
    """
    # é‡å°ä¸åŒå€åŸŸè¨­å®šç²¾æº–çš„ RSS URL
    rss_url = f"https://news.google.com/rss/search?q={keyword}&hl=zh-TW&gl=TW&ceid=TW:zh-Hant"
    
    # æŠ“å–æ–°è
    feed = feedparser.parse(rss_url)
    news_items = []
    
    # ç¢ºä¿æŠ“å–å‰ 20 å‰‡ï¼Œä¸”å…§å®¹ä¸é‡è¤‡
    for entry in feed.entries[:20]:
        news_items.append({
            "title": entry.title,
            "link": entry.link,
            "published": entry.published if hasattr(entry, 'published') else "æœ€æ–°å‹•æ…‹",
            "source": entry.source.title if hasattr(entry, 'source') else "æ¬Šå¨åª’é«”",
            "summary": entry.summary if hasattr(entry, 'summary') else ""
        })
    return news_items

# å®šç¾©åˆ†é èˆ‡é—œéµå­—
tabs = st.tabs(["ğŸ‡¯ğŸ‡µ ç¾æ—¥å°", "ğŸ‡¨ğŸ‡³ ä¸­åœ‹/äºå¤ª", "ğŸ‡·ğŸ‡º ä¿„ç¾…æ–¯/æ­æ´²", "ğŸ‡®ğŸ‡· ä¸­æ±/å…¨çƒ"])
# ç²¾é¸åœ°ç·£æ”¿æ²»èˆ‡è²¡ç¶“é—œéµå­—ï¼Œç¢ºä¿æ–°èå“è³ª
keywords = [
    "ç¾æ—¥å°+åœ°ç·£æ”¿æ²»+åŠå°é«”", 
    "ä¸­åœ‹+äºå¤ªç¶“æ¿Ÿ+è²¿æ˜“è¡çª", 
    "ä¿„ç¾…æ–¯+çƒå…‹è˜­+èƒ½æºå±€å‹¢", 
    "ä¸­æ±+çŸ³æ²¹+å…¨çƒé‡‘è"
]

for idx, tab in enumerate(tabs):
    with tab:
        with st.spinner(f'æ­£åœ¨èˆ‡å…¨çƒæ–°èç¶²åŒæ­¥ä¸­...'):
            news_list = fetch_google_news_rss(keywords[idx])
            
            if not news_list:
                st.error("âš ï¸ åµæ¸¬åˆ°ç¶²è·¯å°é–ï¼Œè«‹å˜—è©¦é‡æ–°æ•´ç†é é¢ã€‚")
            else:
                for n in news_list:
                    # æ¯å‰‡æ–°èéƒ½ä»¥ Expander å±•é–‹ï¼ŒåŒ…å« 200 å­—ä»¥ä¸Šæ·±åº¦æ‘˜è¦ (è‹¥ RSS æä¾›)
                    with st.expander(f"â— {n['title']}", expanded=False):
                        st.markdown(f"**ã€æƒ…å ±ä¾†æºã€‘** {n['source']}  |  **ã€ç™¼å¸ƒæ™‚é–“ã€‘** {n['published']}")
                        st.markdown("---")
                        # é¡¯ç¤ºæ–°èæ‘˜è¦ï¼Œè‹¥æ‘˜è¦éçŸ­å‰‡å¼•å°è‡³é€£çµ
                        clean_summary = n['summary'].split('<')[0] # å»é™¤ HTML æ¨™ç±¤
                        st.write(f"**å¯¦æ™‚å‹•æ…‹ï¼š** {clean_summary}...")
                        st.info("å› ç‰ˆæ¬Šèˆ‡å®‰å…¨æ€§é™åˆ¶ï¼Œæ·±åº¦åˆ†æè«‹é»æ“Šä¸‹æ–¹æ¬Šå¨å ±å°é€£çµé–±è®€ã€‚")
                        st.markdown(f"[ğŸ”— é–±è®€åœ‹éš›åª’é«”åŸå§‹å ±å°å…§å®¹]({n['link']})")
